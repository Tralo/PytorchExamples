{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 MNIST数据集手写数字识别\n",
    "### 3.2.1 数据集介绍\n",
    "MNIST 包括6万张28x28的训练样本，1万张测试样本，很多教程都会对它”下手”几乎成为一个 “典范”，可以说它就是计算机视觉里面的Hello World。所以我们这里也会使用MNIST来进行实战。\n",
    "\n",
    "前面在介绍卷积神经网络的时候说到过LeNet-5，LeNet-5之所以强大就是因为在当时的环境下将MNIST数据的识别率提高到了99%，这里我们也自己从头搭建一个卷积神经网络，也达到99%的准确率\n",
    "\n",
    "### 3.2.2 手写数字识别\n",
    "首先，我们定义一些超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=512 #大概需要2G的显存\n",
    "EPOCHS=20 # 总共训练批次\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 让torch判断是否使用GPU，建议使用GPU环境，因为会快很多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为Pytorch里面包含了MNIST的数据集，所以我们这里直接使用即可。 如果第一次执行会生成data文件夹，并且需要一些时间下载，如果以前下载过就不会再次下载了\n",
    "\n",
    "由于官方已经实现了dataset，所以这里可以直接使用DataLoader来对数据进行读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 9912320/9912422 [08:59<00:00, 37425.96it/s]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/28881 [00:01<?, ?it/s]\u001b[A\n",
      " 57%|█████▋    | 16384/28881 [00:02<00:00, 28461.33it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 16384/1648877 [00:00<00:30, 53551.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 32768/1648877 [00:01<00:29, 53949.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 40960/1648877 [00:01<00:53, 29894.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 65536/1648877 [00:02<00:42, 37060.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 73728/1648877 [00:02<01:02, 25223.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 90112/1648877 [00:02<00:51, 30329.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 106496/1648877 [00:03<00:51, 29822.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 122880/1648877 [00:04<00:51, 29516.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 131072/1648877 [00:05<01:39, 15282.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 147456/1648877 [00:06<01:33, 16045.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "9920512it [09:10, 37425.96it/s]                             [A\u001b[A\n",
      "\n",
      " 10%|▉         | 163840/1648877 [00:08<02:52, 8617.93it/s] \u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 172032/1648877 [00:09<02:15, 10914.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 180224/1648877 [00:09<02:20, 10488.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 188416/1648877 [00:10<02:07, 11432.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 196608/1648877 [00:11<02:40, 9059.97it/s] \u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 204800/1648877 [00:12<02:26, 9841.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 212992/1648877 [00:13<02:11, 10878.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 221184/1648877 [00:13<01:46, 13368.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 229376/1648877 [00:13<01:43, 13672.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 237568/1648877 [00:14<01:26, 16236.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 245760/1648877 [00:14<01:15, 18685.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 253952/1648877 [00:14<01:21, 17195.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 262144/1648877 [00:15<01:05, 21333.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▋        | 270336/1648877 [00:15<01:21, 16820.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 278528/1648877 [00:16<01:29, 15295.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 286720/1648877 [00:16<01:12, 18763.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 294912/1648877 [00:17<01:04, 20914.91it/s]\u001b[A\u001b[A\n",
      "32768it [00:20, 28461.33it/s]                           \u001b[A\n",
      "\n",
      " 18%|█▊        | 303104/1648877 [00:18<02:00, 11179.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 311296/1648877 [00:20<02:59, 7470.39it/s] \u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 319488/1648877 [00:21<02:32, 8729.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 327680/1648877 [00:21<02:13, 9911.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 335872/1648877 [00:21<01:46, 12345.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 344064/1648877 [00:22<01:44, 12428.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██▏       | 352256/1648877 [00:23<01:57, 11060.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 360448/1648877 [00:24<01:47, 11973.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 368640/1648877 [00:24<01:45, 12084.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 376832/1648877 [00:25<01:40, 12696.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 385024/1648877 [00:25<01:36, 13157.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 393216/1648877 [00:26<01:19, 15702.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 401408/1648877 [00:26<01:08, 18152.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 409600/1648877 [00:27<01:30, 13717.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 417792/1648877 [00:27<01:28, 13933.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 425984/1648877 [00:28<01:26, 14072.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▋       | 434176/1648877 [00:29<01:25, 14171.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 442368/1648877 [00:29<01:12, 16732.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 450560/1648877 [00:29<01:14, 15978.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 458752/1648877 [00:30<01:04, 18425.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 466944/1648877 [00:30<00:57, 20677.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 475136/1648877 [00:30<00:51, 22618.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 483328/1648877 [00:31<01:00, 19315.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██▉       | 491520/1648877 [00:31<00:53, 21483.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 499712/1648877 [00:31<00:51, 22428.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 507904/1648877 [00:32<00:45, 24975.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███▏      | 516096/1648877 [00:32<00:43, 26004.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 524288/1648877 [00:32<00:37, 29933.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 532480/1648877 [00:32<00:32, 34460.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 540672/1648877 [00:33<00:34, 32524.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 548864/1648877 [00:33<00:35, 31321.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 557056/1648877 [00:33<00:35, 30530.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 565248/1648877 [00:33<00:36, 29984.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 573440/1648877 [00:34<00:47, 22666.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 589824/1648877 [00:34<00:39, 27030.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▋      | 598016/1648877 [00:35<00:47, 22299.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 606208/1648877 [00:35<00:43, 23919.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 614400/1648877 [00:36<00:44, 23077.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 622592/1648877 [00:36<00:52, 19555.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 630784/1648877 [00:37<01:20, 12598.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 638976/1648877 [00:38<01:04, 15656.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 647168/1648877 [00:38<01:12, 13743.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 655360/1648877 [00:39<01:00, 16288.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 663552/1648877 [00:39<01:02, 15679.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 671744/1648877 [00:39<00:53, 18147.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 679936/1648877 [00:40<00:48, 19830.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 688128/1648877 [00:40<00:46, 20765.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 696320/1648877 [00:40<00:38, 24903.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 704512/1648877 [00:41<00:46, 20424.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 712704/1648877 [00:41<00:41, 22447.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 737280/1648877 [00:42<00:38, 23449.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 753664/1648877 [00:43<00:35, 24867.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 770048/1648877 [00:43<00:35, 24964.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 778240/1648877 [00:45<01:12, 12055.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 786432/1648877 [00:45<01:05, 13168.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 794624/1648877 [00:46<01:03, 13525.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▊     | 802816/1648877 [00:47<01:10, 12049.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 811008/1648877 [00:47<01:07, 12435.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 819200/1648877 [00:48<01:15, 11027.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 827392/1648877 [00:49<01:09, 11854.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 835584/1648877 [00:49<01:04, 12532.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 843776/1648877 [00:50<01:12, 11168.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 851968/1648877 [00:51<01:14, 10661.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 860160/1648877 [00:52<01:08, 11552.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 868352/1648877 [00:53<01:12, 10699.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 876544/1648877 [00:53<01:09, 11055.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▎    | 884736/1648877 [00:54<00:59, 12837.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 892928/1648877 [00:54<00:53, 14023.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 901120/1648877 [00:55<00:48, 15529.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 909312/1648877 [00:55<00:41, 18010.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 917504/1648877 [00:56<00:51, 14272.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 925696/1648877 [00:57<00:59, 12246.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 933888/1648877 [00:57<00:55, 12836.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 942080/1648877 [00:57<00:47, 15012.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 950272/1648877 [00:58<00:39, 17510.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 958464/1648877 [00:59<00:54, 12646.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▊    | 966656/1648877 [00:59<00:44, 15201.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 974848/1648877 [00:59<00:38, 17448.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████▉    | 983040/1648877 [01:01<00:59, 11170.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 991232/1648877 [01:02<01:01, 10653.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 999424/1648877 [01:02<00:59, 10997.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 1007616/1648877 [01:03<00:54, 11832.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 1015808/1648877 [01:04<00:57, 11073.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 1024000/1648877 [01:04<00:51, 12040.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 1032192/1648877 [01:05<00:46, 13204.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 1040384/1648877 [01:05<00:42, 14255.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▎   | 1048576/1648877 [01:06<00:48, 12430.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 1056768/1648877 [01:07<00:45, 12969.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 1064960/1648877 [01:07<00:43, 13360.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 1073152/1648877 [01:08<00:36, 15560.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 1081344/1648877 [01:08<00:36, 15496.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 1089536/1648877 [01:08<00:31, 17967.97it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1097728/1648877 [01:10<00:47, 11597.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 1105920/1648877 [01:10<00:38, 14135.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 1114112/1648877 [01:10<00:32, 16678.57it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 1122304/1648877 [01:11<00:38, 13670.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▊   | 1130496/1648877 [01:11<00:33, 15433.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 1138688/1648877 [01:12<00:28, 17922.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████▉   | 1146880/1648877 [01:12<00:31, 15905.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 1155072/1648877 [01:13<00:24, 20108.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 1163264/1648877 [01:13<00:24, 20049.69it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 1171456/1648877 [01:14<00:27, 17247.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 1179648/1648877 [01:14<00:31, 15027.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 1187840/1648877 [01:16<00:49, 9381.80it/s] \u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 1196032/1648877 [01:17<00:47, 9453.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 1204224/1648877 [01:18<00:56, 7926.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▎  | 1212416/1648877 [01:20<01:00, 7217.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 1220608/1648877 [01:21<01:03, 6713.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▍  | 1228800/1648877 [01:22<00:52, 7994.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 1236992/1648877 [01:23<01:00, 6793.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 1245184/1648877 [01:24<00:57, 7046.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 1253376/1648877 [01:25<00:48, 8137.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 1261568/1648877 [01:26<00:48, 7995.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 1269760/1648877 [01:27<00:41, 9118.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 1277952/1648877 [01:27<00:33, 11205.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 1286144/1648877 [01:27<00:29, 12319.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 1294336/1648877 [01:29<00:39, 9007.79it/s] \u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 1302528/1648877 [01:29<00:34, 10140.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 1310720/1648877 [01:30<00:31, 10760.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████▉  | 1318912/1648877 [01:31<00:28, 11644.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 1327104/1648877 [01:31<00:26, 12365.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 1335296/1648877 [01:32<00:21, 14492.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████▏ | 1343488/1648877 [01:32<00:24, 12574.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 1351680/1648877 [01:33<00:19, 15574.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 1359872/1648877 [01:33<00:16, 17470.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 1368064/1648877 [01:33<00:16, 17170.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 1376256/1648877 [01:34<00:15, 17625.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 1384448/1648877 [01:34<00:15, 17369.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 1392640/1648877 [01:35<00:17, 14927.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▍ | 1400832/1648877 [01:35<00:14, 17452.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 1409024/1648877 [01:36<00:18, 12694.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▋ | 1425408/1648877 [01:37<00:14, 15245.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 1433600/1648877 [01:38<00:14, 14395.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 1441792/1648877 [01:38<00:13, 15316.57it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 1449984/1648877 [01:39<00:11, 16603.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 1458176/1648877 [01:39<00:10, 19006.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 1466368/1648877 [01:39<00:08, 20528.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 1474560/1648877 [01:40<00:08, 20289.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████▉ | 1482752/1648877 [01:40<00:07, 20929.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 1490944/1648877 [01:41<00:08, 17998.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████▏| 1507328/1648877 [01:41<00:06, 20289.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 1515520/1648877 [01:42<00:07, 18056.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 1523712/1648877 [01:42<00:06, 20353.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 1531904/1648877 [01:42<00:05, 19615.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 1540096/1648877 [01:43<00:07, 14951.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 1548288/1648877 [01:44<00:06, 14769.69it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 1556480/1648877 [01:44<00:06, 14640.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▍| 1564672/1648877 [01:45<00:05, 16012.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 1572864/1648877 [01:46<00:06, 11906.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 1581056/1648877 [01:47<00:05, 11484.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▋| 1589248/1648877 [01:49<00:08, 7072.27it/s] \u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 1597440/1648877 [01:49<00:06, 8347.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 1605632/1648877 [01:50<00:04, 9555.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 1613824/1648877 [01:51<00:03, 9560.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 1622016/1648877 [01:51<00:02, 11958.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 1630208/1648877 [01:52<00:01, 12804.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 1638400/1648877 [01:52<00:00, 15354.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|█████████▉| 1646592/1648877 [01:52<00:00, 17437.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "8192it [00:00, 8855.34it/s]             \u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们定义一个网络，网络包含两个卷积层，conv1和conv2，然后紧接着两个线性层作为输出，最后输出10个维度，这10个维度我们作为0-9的标识来确定识别出的是那个数字\n",
    "\n",
    "在这里建议大家将每一层的输入和输出维度都作为注释标注出来，这样后面阅读代码的会方便很多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # batch*1*28*28（每次会送入batch个样本，输入通道数1（黑白图像），图像分辨率是28x28）\n",
    "        # 下面的卷积层Conv2d的第一个参数指输入通道数，第二个参数指输出通道数，第三个参数指卷积核的大小\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5) # 输入通道数1，输出通道数10，核的大小5\n",
    "        self.conv2 = nn.Conv2d(10, 20, 3) # 输入通道数10，输出通道数20，核的大小3\n",
    "        # 下面的全连接层Linear的第一个参数指输入通道数，第二个参数指输出通道数\n",
    "        self.fc1 = nn.Linear(20*10*10, 500) # 输入通道数是2000，输出通道数是500\n",
    "        self.fc2 = nn.Linear(500, 10) # 输入通道数是500，输出通道数是10，即10分类\n",
    "    def forward(self,x):\n",
    "        in_size = x.size(0) # 在本例中in_size=512，也就是BATCH_SIZE的值。输入的x可以看成是512*1*28*28的张量。\n",
    "        out = self.conv1(x) # batch*1*28*28 -> batch*10*24*24（28x28的图像经过一次核为5x5的卷积，输出变为24x24）\n",
    "        out = F.relu(out) # batch*10*24*24（激活函数ReLU不改变形状））\n",
    "        out = F.max_pool2d(out, 2, 2) # batch*10*24*24 -> batch*10*12*12（2*2的池化层会减半）\n",
    "        out = self.conv2(out) # batch*10*12*12 -> batch*20*10*10（再卷积一次，核的大小是3）\n",
    "        out = F.relu(out) # batch*20*10*10\n",
    "        out = out.view(in_size, -1) # batch*20*10*10 -> batch*2000（out的第二维是-1，说明是自动推算，本例中第二维是20*10*10）\n",
    "        out = self.fc1(out) # batch*2000 -> batch*500\n",
    "        out = F.relu(out) # batch*500\n",
    "        out = self.fc2(out) # batch*500 -> batch*10\n",
    "        out = F.log_softmax(out, dim=1) # 计算log(softmax(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们实例化一个网络，实例化后使用.to方法将网络移动到GPU\n",
    "\n",
    "优化器我们也直接选择简单暴力的Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面定义一下训练的函数，我们将训练的所有操作都封装到这个函数中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx+1)%30 == 0: \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试的操作也一样封装成一个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # 将一批的损失相加\n",
    "            pred = output.max(1, keepdim=True)[1] # 找到概率最大的下标\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面开始训练，这里就体现出封装起来的好处了，只要写两行就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [14848/60000 (25%)]\tLoss: 0.317096\n",
      "Train Epoch: 1 [30208/60000 (50%)]\tLoss: 0.259162\n",
      "Train Epoch: 1 [45568/60000 (75%)]\tLoss: 0.172759\n",
      "\n",
      "Test set: Average loss: 0.0985, Accuracy: 9718/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [14848/60000 (25%)]\tLoss: 0.082042\n",
      "Train Epoch: 2 [30208/60000 (50%)]\tLoss: 0.136471\n",
      "Train Epoch: 2 [45568/60000 (75%)]\tLoss: 0.071412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1654784it [02:08, 17437.46it/s]                             \u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0509, Accuracy: 9830/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [14848/60000 (25%)]\tLoss: 0.059068\n",
      "Train Epoch: 3 [30208/60000 (50%)]\tLoss: 0.064005\n",
      "Train Epoch: 3 [45568/60000 (75%)]\tLoss: 0.040977\n",
      "\n",
      "Test set: Average loss: 0.0533, Accuracy: 9825/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [14848/60000 (25%)]\tLoss: 0.043397\n",
      "Train Epoch: 4 [30208/60000 (50%)]\tLoss: 0.033014\n",
      "Train Epoch: 4 [45568/60000 (75%)]\tLoss: 0.020077\n",
      "\n",
      "Test set: Average loss: 0.0404, Accuracy: 9864/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [14848/60000 (25%)]\tLoss: 0.024098\n",
      "Train Epoch: 5 [30208/60000 (50%)]\tLoss: 0.034663\n",
      "Train Epoch: 5 [45568/60000 (75%)]\tLoss: 0.025424\n",
      "\n",
      "Test set: Average loss: 0.0376, Accuracy: 9865/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [14848/60000 (25%)]\tLoss: 0.011488\n",
      "Train Epoch: 6 [30208/60000 (50%)]\tLoss: 0.026061\n",
      "Train Epoch: 6 [45568/60000 (75%)]\tLoss: 0.016886\n",
      "\n",
      "Test set: Average loss: 0.0332, Accuracy: 9899/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [14848/60000 (25%)]\tLoss: 0.010128\n",
      "Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.018336\n",
      "Train Epoch: 7 [45568/60000 (75%)]\tLoss: 0.018728\n",
      "\n",
      "Test set: Average loss: 0.0332, Accuracy: 9888/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [14848/60000 (25%)]\tLoss: 0.006482\n",
      "Train Epoch: 8 [30208/60000 (50%)]\tLoss: 0.012721\n",
      "Train Epoch: 8 [45568/60000 (75%)]\tLoss: 0.022712\n",
      "\n",
      "Test set: Average loss: 0.0336, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [14848/60000 (25%)]\tLoss: 0.008767\n",
      "Train Epoch: 9 [30208/60000 (50%)]\tLoss: 0.012076\n",
      "Train Epoch: 9 [45568/60000 (75%)]\tLoss: 0.026090\n",
      "\n",
      "Test set: Average loss: 0.0359, Accuracy: 9893/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [14848/60000 (25%)]\tLoss: 0.010181\n",
      "Train Epoch: 10 [30208/60000 (50%)]\tLoss: 0.003494\n",
      "Train Epoch: 10 [45568/60000 (75%)]\tLoss: 0.014832\n",
      "\n",
      "Test set: Average loss: 0.0362, Accuracy: 9881/10000 (99%)\n",
      "\n",
      "Train Epoch: 11 [14848/60000 (25%)]\tLoss: 0.003139\n",
      "Train Epoch: 11 [30208/60000 (50%)]\tLoss: 0.028025\n",
      "Train Epoch: 11 [45568/60000 (75%)]\tLoss: 0.014773\n",
      "\n",
      "Test set: Average loss: 0.0321, Accuracy: 9902/10000 (99%)\n",
      "\n",
      "Train Epoch: 12 [14848/60000 (25%)]\tLoss: 0.003680\n",
      "Train Epoch: 12 [30208/60000 (50%)]\tLoss: 0.007328\n",
      "Train Epoch: 12 [45568/60000 (75%)]\tLoss: 0.007955\n",
      "\n",
      "Test set: Average loss: 0.0330, Accuracy: 9899/10000 (99%)\n",
      "\n",
      "Train Epoch: 13 [14848/60000 (25%)]\tLoss: 0.005806\n",
      "Train Epoch: 13 [30208/60000 (50%)]\tLoss: 0.003361\n",
      "Train Epoch: 13 [45568/60000 (75%)]\tLoss: 0.008378\n",
      "\n",
      "Test set: Average loss: 0.0371, Accuracy: 9890/10000 (99%)\n",
      "\n",
      "Train Epoch: 14 [14848/60000 (25%)]\tLoss: 0.015088\n",
      "Train Epoch: 14 [30208/60000 (50%)]\tLoss: 0.005868\n",
      "Train Epoch: 14 [45568/60000 (75%)]\tLoss: 0.002156\n",
      "\n",
      "Test set: Average loss: 0.0367, Accuracy: 9900/10000 (99%)\n",
      "\n",
      "Train Epoch: 15 [14848/60000 (25%)]\tLoss: 0.003204\n",
      "Train Epoch: 15 [30208/60000 (50%)]\tLoss: 0.007293\n",
      "Train Epoch: 15 [45568/60000 (75%)]\tLoss: 0.002133\n",
      "\n",
      "Test set: Average loss: 0.0346, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "Train Epoch: 16 [14848/60000 (25%)]\tLoss: 0.003879\n",
      "Train Epoch: 16 [30208/60000 (50%)]\tLoss: 0.004169\n",
      "Train Epoch: 16 [45568/60000 (75%)]\tLoss: 0.001955\n",
      "\n",
      "Test set: Average loss: 0.0380, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "Train Epoch: 17 [14848/60000 (25%)]\tLoss: 0.003857\n",
      "Train Epoch: 17 [30208/60000 (50%)]\tLoss: 0.001249\n",
      "Train Epoch: 17 [45568/60000 (75%)]\tLoss: 0.000995\n",
      "\n",
      "Test set: Average loss: 0.0403, Accuracy: 9885/10000 (99%)\n",
      "\n",
      "Train Epoch: 18 [14848/60000 (25%)]\tLoss: 0.000429\n",
      "Train Epoch: 18 [30208/60000 (50%)]\tLoss: 0.002839\n",
      "Train Epoch: 18 [45568/60000 (75%)]\tLoss: 0.001484\n",
      "\n",
      "Test set: Average loss: 0.0381, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "Train Epoch: 19 [14848/60000 (25%)]\tLoss: 0.001742\n",
      "Train Epoch: 19 [30208/60000 (50%)]\tLoss: 0.003870\n",
      "Train Epoch: 19 [45568/60000 (75%)]\tLoss: 0.002055\n",
      "\n",
      "Test set: Average loss: 0.0452, Accuracy: 9884/10000 (99%)\n",
      "\n",
      "Train Epoch: 20 [14848/60000 (25%)]\tLoss: 0.001631\n",
      "Train Epoch: 20 [30208/60000 (50%)]\tLoss: 0.004388\n",
      "Train Epoch: 20 [45568/60000 (75%)]\tLoss: 0.002116\n",
      "\n",
      "Test set: Average loss: 0.0446, Accuracy: 9892/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    test(model, DEVICE, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看一下结果，准确率99%，没问题\n",
    "\n",
    "如果你的模型连MNIST都搞不定，那么你的模型没有任何的价值\n",
    "\n",
    "即使你的模型搞定了MNIST，你的模型也可能没有任何的价值\n",
    "\n",
    "MNIST是一个很简单的数据集，由于它的局限性只能作为研究用途，对实际应用带来的价值非常有限。但是通过这个例子，我们可以完全了解一个实际项目的工作流程\n",
    "\n",
    "我们找到数据集，对数据做预处理，定义我们的模型，调整超参数，测试训练，再通过训练结果对超参数进行调整或者对模型进行调整。\n",
    "\n",
    "并且通过这个实战我们已经有了一个很好的模板，以后的项目都可以以这个模板为样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
